<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[红黑树]]></title>
    <url>%2F2018%2F08%2F17%2F%E7%BA%A2%E9%BB%91%E6%A0%91%2F</url>
    <content type="text"><![CDATA[红黑树是具有着色性质的二叉查找树。平均红黑树和评级AVL树一样深，从而查找时间一般接近最优。同时红黑树执行插入操作时，产生的旋转相对较少，性能更优。 特性 每一个节点或者是红色，或者是黑色 根是黑色 如果一个节点是红色的，那么它的子节点必须是黑色的 任何一个节点向下遍历到其子孙的叶子节点，所经过的黑节点个数必须相等 null节点为黑色 旋转左旋 左旋做了三件事： 将y的左子节点赋给x的右子节点,并将x赋给y左子节点的父节点(y左子节点非空时) 将x的父节点p(非空时)赋给y的父节点，同时更新p的子节点为y(左或右) 将y的左子节点设为x，将x的父节点设为y 右旋 右旋做了三件事： 将x的右子节点赋给y的左子节点,并将y赋给x右子节点的父节点(x右子节点非空时) 将y的父节点p(非空时)赋给x的父节点，同时更新p的子节点为x(左或右) 将x的右子节点设为y，将y的父节点设为x 插入首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色，这样就不会违背特性4；最后，通过旋转和重新着色等方法来修正该树，使之重新成为一颗红黑树。 新插入的节点分为三种情况： 新插入的节点是根节点，则直接着色为黑色 新插入的节点的父节点是黑色，则不需要做任何操作，就满足红黑树特性 新插入的节点的父节点是红色，则需要进行调整，使得结构满足红黑树特性，此时也有三种情况： 1 当前节点的父节点是红色，并且其叔叔节点也是红色 1、将父节点设为黑色；2、将叔叔节点设为黑色；3、将爷爷节点设为红色；4、将爷爷节点设为当前节点，继续操作 2 当前节点的父节点是红色，叔叔节点是黑色，且当前节点、父节点、爷爷节点成一字型 1、父节点设为黑色，爷爷节点设为红色；2、对爷爷节点进行单旋 3 当前节点的父节点是红色，叔叔节点是黑色，且当前节点、父节点、爷爷节点成之字型 1、将当前节点设为黑色，爷爷节点设为红色；2、对爷爷节点进行双旋 删除删除操作会删除对应的节点，如果是叶子节点就直接删除，如果是非叶子节点，会用对应的中序遍历的后继节点来顶替要删除节点的位置。删除后就需要做删除修复操作，使的树符合红黑树的定义，符合定义的红黑树高度是平衡的。 红黑树的删除操作是最复杂的操作，复杂的地方就在于当删除了黑色节点的时候，如何从兄弟节点去借调节点，以保证树的颜色符合定义。由于红色的兄弟节点是没法借调出黑节点的，这样只能通过选择操作让他上升到父节点，而由于它是红节点，所以它的子节点就是黑的，可以借调。 对于兄弟节点是黑色节点的可以分成3种情况来处理，当所以的兄弟节点的子节点都是黑色节点时，可以直接将兄弟节点变红，这样局部的红黑树颜色是符合定义的。但是整颗树不一定是符合红黑树定义的，需要往上追溯继续调整。 对于兄弟节点的子节点为左红右黑或者 (全部为红，右红左黑)这两种情况，可以先将前面的情况通过选择转换为后一种情况，在后一种情况下，因为兄弟节点为黑，兄弟节点的右节点为红，可以借调出两个节点出来做黑节点，这样就可以保证删除了黑节点，整棵树还是符合红黑树的定义的，因为黑色节点的个数没有改变。 红黑树的删除操作是遇到删除的节点为红色，或者追溯调整到了root节点，这时删除的修复操作完毕。 代码实现https://github.com/fup1990/EasyRPC/blob/master/nameserver/src/main/java/com/gome/fup/easy/rpc/nameserver/data/NodeTree.java]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树]]></title>
    <url>%2F2018%2F08%2F17%2F%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[二叉树二叉树是一棵树，其中每个节点都不能有多于两个的字节点。 二叉查找树对于树中的每一个节点X，它的左子树中所有项的值小于X中的项，而它的右子树中所有项的值大于X中的项。这意味这该树所有的元素可以用某种一致的方式排序。 插入二分法遍历树中的节点，如果新插入的节点X的key不存在，则插入为叶子节点，如果存在则更新。 删除当删除节点X时，需要考虑以下几种情况： 若X是叶子节点：直接删除； 若X有一个子节点：则让X的子节点，代替X成为X父节点的子节点； 若X有两个子节点：让X的右子树的最小的节点Y代替X成为X父节点的子节点，并递归的删除原来的Y节点 如果删除的次数不频繁，可以使用懒惰删除的策略：当一个节点要被删除时，它仍留在树中，只是被标记删除。 遍历 前序排序根结点 —&gt; 左子树 —&gt; 右子树 前序遍历的输出结果：ABDECF 中序排序左子树—&gt; 根结点 —&gt; 右子树 中序遍历的输出结果：DBEAFC 后序排序左子树 —&gt; 右子树 —&gt; 根结点 前序遍历的输出结果：DEBFCA 平衡二叉查找树一棵平衡二叉查找树是其每个节点的左子树和右子树的高度最多差1的二叉查找树。 旋转单旋当新插入节点和其父节点、祖父节点成一条直线时，需要执行单旋操作。单旋分为左旋和右旋。 双旋当新插入节点和其父节点、祖父节点不在一条直线上，成之子型，需要执行双旋操作。根据新插入节点所处的位置，双旋分为： 左-右双旋，即先左旋后右旋 右-左双旋，即先右旋后左旋 插入定义 AVL树：T 新插入节点：X 递归的将X插入到T的相应的子树：t t树中高度较高一侧的子节点：c 插入操作分为几种情况： 插入X之后，t的左右高度差小于2，则表示此次插入没有破坏T的结构； 插入X之后，t的左右高度差等于2 当c是t的左子节点，且X的值小于c的值时，对t进行右旋； 当c是t的左子节点，且X的值大于c小于t时，对t进行左-右双旋； 当c是t的右子节点，且X的值大于c的值时，对t进行左旋； 当c是t的右子节点，且X的值大于t小于c时，对t进行右-左双旋；]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CPU缓存]]></title>
    <url>%2F2018%2F08%2F16%2FCPU%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[CPU缓存随着CPU的频率不断提升，而内存的访问速度却没有质的突破，为了弥补访问内存的速度慢，充分发挥CPU的计算资源，提高CPU整体吞吐量，在CPU与内存之间引入了一级Cache。 一般来说，一级Cache可以分为一级数据缓存（Data Cache，D-Cache）和一级指令缓存（Instruction Cache，I-Cache）。随着热点数据体积越来越大，一级Cache L1已经不满足发展的要求，引入了二级Cache L2，三级Cache L3。 缓存行CPU缓存是由很多个缓存行组成的。每个缓存行通常是64字节，并且它有效地引用主内存中的一块儿地址，并拷贝主内存对应地址中的数据。缓存行是CPU与主内存数据交换的最小单位。CPU每次从主存中拉取数据时，会把相邻的数据也存入同一个缓存行。 伪共享 数据X、Y、Z被加载到同一Cache Line中，线程A在Core1修改X，线程B在Core2上修改Y。根据MESI大法，假设是Core1是第一个发起操作的CPU核，Core1上的L1 Cache Line由S（共享）状态变成M（修改，脏数据）状态，然后告知其他的CPU核，图例则是Core2，引用同一地址的Cache Line已经无效了；当Core2发起写操作时，首先导致Core1将X写回主存，Cache Line状态由M变为I（无效），而后才是Core2从主存重新读取该地址内容，Cache Line状态由I变成E（独占），最后进行修改Y操作， Cache Line从E变成M。可见多个线程操作在同一Cache Line上的不同数据，相互竞争同一Cache Line，导致线程彼此牵制影响，变成了串行程序，降低了并发性。此时我们则需要将共享在多线程间的数据进行隔离，使他们不在同一个Cache Line上，从而提升多线程的性能。 解决伪共享方案 1、填充方式正确的方式应该将该对象属性分组，将一起变化的放在一组，与其他属性无关的属性放到一组，将不变的属性放到一组。这样当每次对象变化时，不会带动所有的属性重新加载缓存，提升了读取效率。在JDK1.8以前，我们一般是在属性间增加长整型变量来分隔每一组属性。 12345678910public class DataPadding&#123; long a1,a2,a3,a4,a5,a6,a7,a8;//防止与前一个对象产生伪共享 int value; long modifyTime; long b1,b2,b3,b4,b5,b6,b7,b8;//防止不相关变量伪共享; boolean flag; long c1,c2,c3,c4,c5,c6,c7,c8;// long createTime; char key; long d1,d2,d3,d4,d5,d6,d7,d8;//防止与下一个对象产生伪共享 2、Contended注解方式在JDK1.8中，新增了一种注解@sun.misc.Contended，来使各个变量在Cache line中分隔开。注意，jvm需要添加参数-XX:-RestrictContended才能开启此功能。 12345678910111213141516171819202122232425// 类前加上代表整个类的每个变量都会在单独的cache line中@sun.misc.Contended@SuppressWarnings("restriction")public class ContendedData &#123; int value; long modifyTime; boolean flag; long createTime; char key;&#125;或者这种：// 属性前加上时需要加上组标签@SuppressWarnings("restriction")public class ContendedGroupData &#123; @sun.misc.Contended("group1") int value; @sun.misc.Contended("group1") long modifyTime; @sun.misc.Contended("group2") boolean flag; @sun.misc.Contended("group3") long createTime; @sun.misc.Contended("group3") char key;&#125;]]></content>
      <categories>
        <category>操作系统原理</category>
      </categories>
      <tags>
        <tag>操作系统原理</tag>
        <tag>性能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AQS]]></title>
    <url>%2F2018%2F08%2F16%2FAQS%2F</url>
    <content type="text"><![CDATA[AQS是AbstractQueuedSynchronizer类的简称，即队列同步器。它是构建锁或者其他同步组件的基础框架。 AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 AQS 使用一个 volatile int 类型的成员变量 state 来表示同步状态： 当 state &gt; 0 时，表示已经获取了锁。 当 state = 0 时，表示释放了锁。 AQS 通过内置的 FIFO 同步队列来完成资源获取线程的排队工作。如果当前线程获取同步状态失败（锁）时，AQS 则会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程 当同步状态释放时，则会把节点中的线程唤醒，使其再次尝试获取同步状态。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池]]></title>
    <url>%2F2018%2F08%2F16%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[ExecutorEexecutor作为灵活且强大的异步执行框架，其支持多种不同类型的任务执行策略，提供了一种标准的方法将任务的提交过程和执行过程解耦开发，基于生产者-消费者模式，其提交任务的线程相当于生产者，执行任务的线程相当于消费者，并用Runnable来表示任务，Executor的实现还提供了对生命周期的支持，以及统计信息收集，应用程序管理机制和性能监视等机制。 ExecutorServiceExecutorService是Executor直接的扩展接口，也是最常用的线程池接口，我们通常见到的线程池定时任务线程池都是它的实现类。 ThreadPoolExecutorjava的线程池支持主要通过ThreadPoolExecutor来实现，我们使用的ExecutorService的各种线程池策略都是基于ThreadPoolExecutor实现的，所以ThreadPoolExecutor十分重要。要弄明白各种线程池策略，必须先弄明白ThreadPoolExecutor。 构造参数说明1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 参数说明： corePoolSize：核心线程数，如果运行的线程少于corePoolSize，则创建新线程来执行新任务，即使线程池中的其他线程是空闲的。 maximumPoolSize：最大线程数，可允许创建的线程数。 keepAliveTime：如果线程数多于corePoolSize,则这些多余的线程的空闲时间超过keepAliveTime时将被终止。 unit：keepAliveTime参数的时间单位。 workQueue：保存任务的阻塞队列。 当运行的线程数少于corePoolSize时，在有新任务时直接创建新线程来执行任务而无需再进队列 ； 当运行的线程数等于或多于corePoolSize，在有新任务添加时则选加入队列，不直接创建线程 ； 当队列满时，在有新任务时就创建新线程。 threadFactory：使用ThreadFactory创建新线程，默认使用defaultThreadFactory创建线程。 handler： 定义处理被拒绝任务的策略，默认使用 ThreadPoolExecutor.AbortPolicy， 抛出RejectExecutorException。 执行任务流程 拒绝策略 ThreadPoolExecutor.AbortPolicy：默认策略，抛出RejectExecutorException。 ThreadPoolExecutor.CallerRunsPolicy：改为本地线程同步执行任务。 ThreadPoolExecutor.DiscardPolicy：丢弃任务。 ThreadPoolExecutor.DiscardOldestPolicy：从阻塞队列中取出队首的任务丢弃，然后推入队列。 生命周期 RUNNING：初始状态，接受新任务并且处理已经在队列中的任务。 SHUTDOWN：不接受新任务，但处理队列中的任务。 STOP：不接受新任务，不处理排队的任务，并中断正在进行的任务。 TIDYING：所有任务已终止，workerCount为零，线程转换到状态TIDYING，这时回调terminate()方法。 TERMINATED：终态，terminated()执行完成。 shutdown()：平缓的关闭。不再接受新的任务，同事等待已经提交的任务执行完成，包括未执行的任务。 shutdownNow()：暴力的关闭。取消所有运行中的任务，并且不再执行队列中尚未执行的任务。 ExecutorsExecutors提供了一系列静态工厂方法用于创建各种线程池。 newFixedThreadPool创建一个固定长度的线程池，每当提交一次任务时就创建一个线程，直到达到线程池的最大数量，这时线程池的规模将不再变化。 1234567public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); &#125; corePoolSize与 maximumPoolSize数量相等，表示线程池将维护固定数量的线程。使用了无界的 LinkedBlockingQueue队列，所以可以一直添加新任务到线程池，不会触发拒绝机制。 newCachedThreadPool创建一个可换成的线程池，如果线程池的当前规模超过了需要处理的任务数量时，那么将回收空闲的线程；而当 需要处理的任务数量增加时，则添加新的线程。线程池的规模不存在任何限制。 1234567public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; corePoolSize为0， maximumPoolSize为 Integer.MAX_VALUE表示线程池容量为无限。闲置60秒的线程将被回收。 SynchronousQueue是一个阻塞的同步队列， 队列只能存储一个元素。因此，线程池会不断创建新的线程，极端场景下会因为线程数量过多而耗尽计算机资源。 newSingleThreadExecutor一个单线程的Executor，创建单个工作线程来执行任务；如果线程异常，则创建另一个线程来替代。 12345678public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; newScheduledThreadPool创建一个固定长度的线程池，而且以延迟或定时的方式来执行任务。 1234public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); &#125; ScheduledExecutorServiceScheduledExecutorService一种安排任务执行的ExecutorService，任务可以延迟执行，或者在一个固定的时间间隔内重复执行。 12//创建定时器线程池ScheduledExecutorService executorService = Executors.newScheduledThreadPool(1); 常用方法 方法说明 重要参数说明 schedule(Runnable command, long delay, TimeUnit unit) 延后指定时间执行任务 delay:延后指定时间执行任务 scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) 周期的执行任务，一次只能执行一个任务，当前一个任务没有执行完，而周期时间到了，则下一个任务等待前一个任务执行完毕之后立即执行 initialDelay:延后指定时间执行任务；period:定期执行 scheduleWithFixedDelay(Runnable command, long initialDelay, long period, TimeUnit unit) 周期的执行任务，当前一个任务执行完毕后，开始计时 initialDelay:延后指定时间执行任务；period:定期执行 设置线程池的大小设置线程池的大小时，应避免过大或过小这两种极端情况。如果线程池过大，那么大量的线程将在相对很少的CPU和内存资源上发生竞争，降低系统性能，耗费服务器资源。如果线程池过小，那么将导致许多空闲的处理器无法执行工作，浪费资源。要想正确的设置线程池的大小，需要分析计算㕂、资源预算和任务特性。 对于计算密集型任务123N = 线程数量C = CPU数量N = C + 1 对于IO密集型任务123456N = 线程数量C = CPU数量P = CPU利用率CT = 计算时间WT = IO等待时间N = C * P * (1 + CT/WT) 当然，CPU周期并不是唯一影响线程池大小饿资源，还包括内存、文件句柄、套接字句柄和数据库连接等。计算这些资源对线程池的约束条件是更容易的：计算每个任务对该资源的需求量，然后用该资源的可用总量除以每个任务的需求量，所得结果就是线程池大小的上线。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap]]></title>
    <url>%2F2018%2F08%2F16%2FHashMap%2F</url>
    <content type="text"><![CDATA[概要HashMap 是一个关联数组、哈希表，它是线程不安全的，允许key为null,value为null。遍历时无序。 其底层数据结构是数组称之为哈希桶，每个桶里面放的是链表，链表中的每个节点，就是哈希表中的每个元素。 在JDK8中，当链表长度达到8，会转化成红黑树，以提升它的查询、插入效率，它实现了Map&lt;K,V&gt;, Cloneable, Serializable接口。 数据结构 构造方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384//最大容量 2的30次方static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//默认的加载因子static final float DEFAULT_LOAD_FACTOR = 0.75f;//哈希桶，存放链表。 长度是2的N次方，或者初始化时为0.transient Node&lt;K,V&gt;[] table;//加载因子，用于计算哈希表元素数量的阈值。 threshold = 哈希桶.length * loadFactor;final float loadFactor;//哈希表内元素数量的阈值，当哈希表内元素数量超过阈值时，会发生扩容resize()。int threshold;public HashMap() &#123; //默认构造函数，赋值加载因子为默认的0.75f this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;public HashMap(int initialCapacity) &#123; //指定初始化容量的构造函数 this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;//同时指定初始化容量 以及 加载因子， 用的很少，一般不会修改loadFactorpublic HashMap(int initialCapacity, float loadFactor) &#123; //边界处理 if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); //初始容量最大不能超过2的30次方 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //显然加载因子不能为负数 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; //设置阈值为 》=初始化容量的 2的n次方的值 this.threshold = tableSizeFor(initialCapacity);&#125;//新建一个哈希表，同时将另一个map m 里的所有元素加入表中public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125;//根据期望容量cap，返回2的n次方形式的 哈希桶的实际容量 length。 返回值一般会&gt;=cap static final int tableSizeFor(int cap) &#123;//经过下面的 或 和位移 运算， n最终各位都是1。 int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; //判断n是否越界，返回 2的n次方作为 table（哈希桶）的阈值 return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125;//将另一个Map的所有元素加入表中，参数evict初始化时为false，其他情况为truefinal void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; //拿到m的元素数量 int s = m.size(); //如果数量大于0 if (s &gt; 0) &#123; //如果当前表是空的 if (table == null) &#123; // pre-size //根据m的元素数量和当前表的加载因子，计算出阈值 float ft = ((float)s / loadFactor) + 1.0F; //修正阈值的边界 不能超过MAXIMUM_CAPACITY int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); //如果新的阈值大于当前阈值 if (t &gt; threshold) //返回一个 》=新的阈值的 满足2的n次方的阈值 threshold = tableSizeFor(t); &#125; //如果当前元素表不是空的，但是 m的元素数量大于阈值，说明一定要扩容。 else if (s &gt; threshold) resize(); //遍历 m 依次将元素加入当前表中。 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; Node&lt;K,V&gt; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; 扩容当HashMap的容量达到threshold域值时，就会触发扩容。扩容前后，哈希桶的长度一定会是2的次方。扩容操作时，会new一个新的Node数组作为哈希桶，然后将原哈希表中的所有数据(Node节点)移动到新的哈希桶中，相当于对原哈希表中所有的数据重新做了一个put操作。所以性能消耗很大，可想而知，在哈希表的容量越大时，性能消耗越明显。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112final Node&lt;K,V&gt;[] resize() &#123; //oldTab 为当前表的哈希桶 Node&lt;K,V&gt;[] oldTab = table; //当前哈希桶的容量 length int oldCap = (oldTab == null) ? 0 : oldTab.length; //当前的阈值 int oldThr = threshold; //初始化新的容量和阈值为0 int newCap, newThr = 0; //如果当前容量大于0 if (oldCap &gt; 0) &#123; //如果当前容量已经到达上限 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; //则设置阈值是2的31次方-1 threshold = Integer.MAX_VALUE; //同时返回当前的哈希桶，不再扩容 return oldTab; &#125;//否则新的容量为旧的容量的两倍。 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY)//如果旧的容量大于等于默认初始容量16 //那么新的阈值也等于旧的阈值的两倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125;//如果当前表是空的，但是有阈值。代表是初始化时指定了容量、阈值的情况 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr;//那么新表的容量就等于旧的阈值 else &#123;&#125;//如果当前表是空的，而且也没有阈值。代表是初始化时没有任何容量/阈值参数的情况 // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;//此时新表的容量为默认的容量 16 //新的阈值为默认容量16 * 默认加载因子0.75f = 12 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123;//如果新的阈值是0，对应的是 当前表是空的，但是有阈值的情况 float ft = (float)newCap * loadFactor;//根据新表容量 和 加载因子 求出新的阈值 //进行越界修复 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //更新阈值 threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) //根据新的容量 构建新的哈希桶 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; //更新哈希桶引用 table = newTab; //如果以前的哈希桶中有元素 //下面开始将当前哈希桶中的所有节点转移到新的哈希桶中 if (oldTab != null) &#123; //遍历老的哈希桶 for (int j = 0; j &lt; oldCap; ++j) &#123; //取出当前的节点 e Node&lt;K,V&gt; e; //如果当前桶中有元素,则将链表赋值给e if ((e = oldTab[j]) != null) &#123; //将原哈希桶置空以便GC oldTab[j] = null; //如果当前链表中就一个元素，（没有发生哈希碰撞） if (e.next == null) //直接将这个元素放置在新的哈希桶里。 //注意这里取下标 是用 哈希值 与 桶的长度-1 。 //由于桶的长度是2的n次方，这么做其实是等于 一个模运算。但是效率更高 newTab[e.hash &amp; (newCap - 1)] = e; //如果发生过哈希碰撞 ,而且是节点数超过8个，转化成了红黑树 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果发生过哈希碰撞，节点数小于8个。 //则要根据链表上每个节点的哈希值，依次放入新哈希桶对应下标位置。 else &#123; // preserve order //因为扩容是容量翻倍，所以原链表上的每个节点。 //现在可能存放在原来的下标，即low位， 或者扩容后的下标，即high位。 //high位= low位+原哈希桶容量 //低位链表的头结点、尾节点 Node&lt;K,V&gt; loHead = null, loTail = null; //高位链表的头节点、尾节点 Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next;//临时节点 存放e的下一个节点 do &#123; next = e.next; //这里又是一个利用位运算 代替常规运算的高效点： //利用哈希值 与 旧的容量，可以得到哈希值去模后， //是大于等于oldCap还是小于oldCap，等于0代表小于oldCap，应该存放在低位， //否则存放在高位 if ((e.hash &amp; oldCap) == 0) &#123; //给头尾节点指针赋值 if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125;//高位也是相同的逻辑 else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125;//循环直到链表结束 &#125; while ((e = next) != null); //将低位链表存放在原index处， if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; //将高位链表存放在新index处 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; putValue12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //tab存放 当前的哈希桶， p用作临时链表节点 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果当前哈希表是空的，代表是初始化 if ((tab = table) == null || (n = tab.length) == 0) //那么直接去扩容哈希表，并且将扩容后的哈希桶长度赋值给n n = (tab = resize()).length; //如果当前index的节点是空的，表示没有发生哈希碰撞。 直接构建一个新节点Node，挂载在index处即可。 //这里再啰嗦一下，index 是利用 哈希值 &amp; 哈希桶的长度-1，替代模运算 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123;//否则 发生了哈希冲突。 Node&lt;K,V&gt; e; K k; //如果哈希值相等，key也相等，则是覆盖value操作 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p;//将当前节点引用赋值给e else if (p instanceof TreeNode)//红黑树 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123;//不是覆盖操作，则插入一个普通链表节点 //遍历链表 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123;//遍历到尾部，追加新节点到尾部 p.next = newNode(hash, key, value, null); //如果追加节点后，链表数量&gt;=8，则转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //如果找到了要覆盖的节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果e不是null，说明有需要覆盖的节点， if (e != null) &#123; // existing mapping for key //则覆盖节点值，并返回原oldValue V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; //这是一个空实现的函数，用作LinkedHashMap重写使用。 afterNodeAccess(e); return oldValue; &#125; &#125; //如果执行到了这里，说明插入了一个新的节点，所以会修改modCount，以及返回null。 //修改modCount ++modCount; //更新size，并判断是否需要扩容。 if (++size &gt; threshold) resize(); //这是一个空实现的函数，用作LinkedHashMap重写使用。 afterNodeInsertion(evict); return null;&#125; 小结 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 红黑树大程度优化了HashMap的性能。]]></content>
      <categories>
        <category>源码</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>数据结构</tag>
        <tag>集合</tag>
      </tags>
  </entry>
</search>
